{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sneaker Resale Prices: Can We Understand the Hype?\n",
    "=====================\n",
    "Sameen Salam- M.S. Candidate, Institute for Advanced Analytics at North Carolina State University                                                                                                                                        \n",
    "Mail: ssalam@ncsu.edu \n",
    "\n",
    "Introduction\n",
    "------------\n",
    "In this notebook, we will explore sneaker transaction data from the online marketplace StockX. StockX put out this dataset as part of its 2019 Data Challenge. The data consists of a random sample of all Off-White and Yeezy 350 sales from between 9/1/2017 (the month that Off-White first debuted “The Ten” collection) and 2/15/2019.\n",
    "\n",
    "This is the modeling notebook. Here is an overview of how this notebook is structured:\n",
    "\n",
    "**The Sneaker Game** - brief explanation on some basic aspects of the sneaker market  \n",
    "**Feature Engineering** - conceptualization and creation of additional variables with modeling in mind  \n",
    "**Modeling** - mapping out the relationships between features and the selected target  \n",
    "**Conclusions** - summarzing modeling results and paths the project could take moving forward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sneaker Game\n",
    "---------------\n",
    "Sneaker culture has become ubiquitous in recent years. All around the world, millions of sneakerheads (myself included) go online to cop the newest releases and rarest classics. But it's not always quite as simple as clicking the \"Add to Cart\" button and making the purchase. Some sneakers have incredibly high demand leading up to a very limited supply upon release. Only a few dedicated and/or lucky people will be successful in purchasing these shoes from a shoe store or website. Some may choose wear their shoes and keep them in their collection for years to come. But many will choose to resell deadstock (brand new) shoes at a profit in order to purchase even rarer ones.\n",
    "\n",
    "This is where StockX, GOAT, FlightClub, or any other online sneaker marketplace comes in. Resellers need to connect with individuals who want the shoes they have up for sale. These entities offer a platform that put resellers in direct contact with potential buyers. StockX in particular prides itself on being the stock market analog in the sneaker world. Resellers can list sneakers for sale at whatever price they see fit, and buyers can make whatever bids or offers they would like on any sneaker. StockX's role in the transaction is to make sure that the resellers are selling authentic sneakers to protect buyers from receiving fake or damaged sneakers. \n",
    "\n",
    "Yeezys and Off-Whites are popular examples of coveted shoes that sneakerheads buy off of one another. The Yeezy line is a collaboration between Adidas and musical artist Kanye West. There are several other sneakers that fall under the Yeezy brand, but this dataset only covers Yeezy Boost 350 models. The Off-White line is a collaboration between Nike and luxury designer Virgil Abloh. Like the Yeezys, this dataset focuses in on a subset of Off-White sneaker models known as \"The Ten\". This is a set of ten different shoes released by Nike over a period of several months. The sneakers that carry these brand labels represent some of the most sought after kicks in the world, selling out in stores and online almost instantly upon release. \n",
    "\n",
    "### Value Proposition\n",
    "After conducting some research on StockX's website, I found that StockX's revenue stream comes primarily from a 3% payment processing fee and a regressive transaction fee (i.e. the more a reseller sells on StockX, the lower your fee per item is). It is in StockX's best interest to foster sales of shoes with higher sale prices from a revenue standpoint. If reasonably accurate predictions can be made on resale prices as they relate to retail prices, then StockX can make decisions promoting certain sneaker listings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup\n",
    "Importing needed libraries and reading in the spreadsheet directly from StockX's 2019 Data Challenge page. Note that this code will download the dataset in the form of an .xlsx spreadsheet in the directory from which this notebook is being run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import pandas as pd\n",
    "from statistics import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.patches as mpatch\n",
    "import matplotlib.dates as mdate\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from regressors import stats as stats_reg\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the data directly from the StockX Data Challenge 2019 webpage\n",
    "url = \"https://s3.amazonaws.com/stockx-sneaker-analysis/wp-content/uploads/2019/02/StockX-Data-Contest-2019-3.xlsx\"\n",
    "my_file = requests.get(url)\n",
    "output = open('StockX-Data-Contest-2019-3.xlsx', 'wb')\n",
    "output.write(my_file.content)\n",
    "output.close()\n",
    "\n",
    "#Save the contents of the excel file into variable: stockx_data\n",
    "stockx_data = pd.read_excel(\"StockX-Data-Contest-2019-3.xlsx\", sheet_name = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Data Check\n",
    "Here we are looking at some basic characteristics and quality of the dataset. Data cleaning was not a significant hurdle in this project because the dataset provided by StockX was very clean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Sneaker Name</th>\n",
       "      <th>Sale Price</th>\n",
       "      <th>Retail Price</th>\n",
       "      <th>Release Date</th>\n",
       "      <th>Shoe Size</th>\n",
       "      <th>Buyer Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>Yeezy</td>\n",
       "      <td>Adidas-Yeezy-Boost-350-Low-V2-Beluga</td>\n",
       "      <td>1097.0</td>\n",
       "      <td>220</td>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>11.0</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>Yeezy</td>\n",
       "      <td>Adidas-Yeezy-Boost-350-V2-Core-Black-Copper</td>\n",
       "      <td>685.0</td>\n",
       "      <td>220</td>\n",
       "      <td>2016-11-23</td>\n",
       "      <td>11.0</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>Yeezy</td>\n",
       "      <td>Adidas-Yeezy-Boost-350-V2-Core-Black-Green</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220</td>\n",
       "      <td>2016-11-23</td>\n",
       "      <td>11.0</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>Yeezy</td>\n",
       "      <td>Adidas-Yeezy-Boost-350-V2-Core-Black-Red</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>220</td>\n",
       "      <td>2016-11-23</td>\n",
       "      <td>11.5</td>\n",
       "      <td>Kentucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>Yeezy</td>\n",
       "      <td>Adidas-Yeezy-Boost-350-V2-Core-Black-Red-2017</td>\n",
       "      <td>828.0</td>\n",
       "      <td>220</td>\n",
       "      <td>2017-02-11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Rhode Island</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order Date   Brand                                   Sneaker Name  \\\n",
       "0 2017-09-01   Yeezy           Adidas-Yeezy-Boost-350-Low-V2-Beluga   \n",
       "1 2017-09-01   Yeezy    Adidas-Yeezy-Boost-350-V2-Core-Black-Copper   \n",
       "2 2017-09-01   Yeezy     Adidas-Yeezy-Boost-350-V2-Core-Black-Green   \n",
       "3 2017-09-01   Yeezy       Adidas-Yeezy-Boost-350-V2-Core-Black-Red   \n",
       "4 2017-09-01   Yeezy  Adidas-Yeezy-Boost-350-V2-Core-Black-Red-2017   \n",
       "\n",
       "   Sale Price  Retail Price Release Date  Shoe Size  Buyer Region  \n",
       "0      1097.0           220   2016-09-24       11.0    California  \n",
       "1       685.0           220   2016-11-23       11.0    California  \n",
       "2       690.0           220   2016-11-23       11.0    California  \n",
       "3      1075.0           220   2016-11-23       11.5      Kentucky  \n",
       "4       828.0           220   2017-02-11       11.0  Rhode Island  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taking a quick look at how the data is structured\n",
    "stockx_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we confirm that the dataset has the correct dimensions: 99,956 rows and 8 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99956, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the dimensions of the data\n",
    "stockx_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output below, we can see that this data contains no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order Date      0\n",
       "Brand           0\n",
       "Sneaker Name    0\n",
       "Sale Price      0\n",
       "Retail Price    0\n",
       "Release Date    0\n",
       "Shoe Size       0\n",
       "Buyer Region    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for missing values\n",
    "stockx_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the datatypes of the columns are congruent with the information they provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order Date      datetime64[ns]\n",
       "Brand                   object\n",
       "Sneaker Name            object\n",
       "Sale Price             float64\n",
       "Retail Price             int64\n",
       "Release Date    datetime64[ns]\n",
       "Shoe Size              float64\n",
       "Buyer Region            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the data types of each column\n",
    "stockx_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **Brand** variable, Yeezys were indicated by \" Yeezy\", making analysis more difficult. In this code, I strip all leading and trailing white space in **Brand** to make the data just a bit cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stripping all leading and trailing white space in the Brand column\n",
    "stockx_data[\"Brand\"] = stockx_data[\"Brand\"].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "99,956 rows and 8 columns\n",
    "    \n",
    "No missing values\n",
    "\n",
    "#### Variables\n",
    "1. **Order Date**: datetime, Date the transaction occurred\n",
    "\n",
    "2. **Brand**: string, Brand of the sneaker\n",
    "\n",
    "3. **Sneaker Name**: string, Name of the sneaker\n",
    "\n",
    "4. **Sale Price**: numpy float, Price paid for sneaker in transaction (resale price)\n",
    "\n",
    "5. **Retail Price**: numpy int, Retail price of sneaker in stores (release price)\n",
    "\n",
    "6. **Release Date**: datetime, Date the sneaker dropped\n",
    "\n",
    "7. **Shoe Size**: numpy float, Shoe size (most likely in mens)\n",
    "\n",
    "8. **Buyer Region**: string, transaction state (most likely for purchaser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "------------------\n",
    "Sometimes the variables in your dataset do not capture the complete story. Feature engineering, or creating new variables based on ones you do have, can allow for clearer or more interesting insights and pave the way for useful models. \n",
    "\n",
    "With potential modeling applications in mind, here is a breakdown of some additional features I want to create: \n",
    "\n",
    "1. date_diff: Elapsed time between **Order Date** and **Release Date**. This allows cross comparisons of different sneakers over the same point in time after release. \n",
    "\n",
    "2. price_ratio: Ratio of **Sale Price** to **Retail Price**. This standardizes each shoe relative to its retail price and acts as a better indicator of the hype of any given sneaker. \n",
    "\n",
    "3. jordan: A binary indicator variable derived from **Sneaker Name** that indicates if the shoe is a Jordan or not. \n",
    "\n",
    "4. V2: A binary indicator variable derived from **Sneaker Name** that indicates if the shoe is a V2 model or not. \n",
    "\n",
    "5. blackcol: A binary indicator variable derived from **Sneaker Name** that indicates if the shoe has black in the colorway or not. \n",
    "\n",
    "6. airmax90: A binary indicator variable derived from **Sneaker Name** that indicates if the shoe is an Airmax 90 or not. \n",
    "\n",
    "7. airmax97: A binary indicator variable derived from **Sneaker Name** that indicates if the shoe is an Airmax 90 or not. \n",
    "\n",
    "8. zoom: A binary indicator variable derived from **Sneaker Name** that indicates if the shoe has Zoom in the name or not. \n",
    "\n",
    "9. presto: A binary indicator variable derived from **Sneaker Name** that indicates if the shoe is a Presto or not. \n",
    "\n",
    "10. airforce: A binary indicator variable derived from **Sneaker Name** that indicates if the shoe is an AirForce or not. \n",
    "\n",
    "11. blazer: A binary indicator variable derived from **Sneaker Name** that indicates if the shoe is a Blazer or not. \n",
    "\n",
    "12. vapormax: A binary indicator variable derived from **Sneaker Name** that indicates if the shoe is an Vapor Max or not. \n",
    "\n",
    "13. california: A binary indicator variable derived from **Buyer Region** that indicates if the buyer is from California or not. \n",
    "\n",
    "14. new_york: A binary indicator variable derived from **Buyer Region** that indicates if the buyer is from New York or not. \n",
    "\n",
    "15. oregon: A binary indicator variable derived from **Buyer Region** that indicates if the buyer is from Oregon or not. \n",
    "\n",
    "16. florida: A binary indicator variable derived from **Buyer Region** that indicates if the buyer is from Florida or not. \n",
    "\n",
    "17. texas:  A binary indicator variable derived from **Buyer Region** that indicates if the buyer is from Texas or not. \n",
    "\n",
    "18. other_state: A binary indicator variable derived from **Buyer Region** that indicates if the buyer is from any other state. The states in this category had <5% of transactions in the data.  \n",
    "\n",
    "19. brand2: A binary indicator variable derived from **Brand** for modeling. Yeezy = 1, Off-White = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the difference between Order Date and Release Date to create a new column: date_diff\n",
    "stockx_data[\"date_diff\"] = stockx_data['Order Date'].sub(stockx_data['Release Date'], axis=0)/np.timedelta64('1','D')\n",
    "\n",
    "#Creating a new column containing the ratio of Sale Price and Retail Price: price_ratio\n",
    "stockx_data[\"price_ratio\"] = stockx_data[\"Sale Price\"]/stockx_data[\"Retail Price\"]\n",
    "\n",
    "#Creating the jordan variable\n",
    "stockx_data[\"jordan\"] = stockx_data['Sneaker Name'].apply(lambda x : 1 if 'Jordan' in x.split(\"-\") else 0)\n",
    "\n",
    "#Creating the V2 variable\n",
    "stockx_data[\"V2\"] = stockx_data['Sneaker Name'].apply(lambda x : 1 if 'V2' in x.split(\"-\") else 0)\n",
    "\n",
    "#Creating the blackcol variable\n",
    "stockx_data[\"blackcol\"] = stockx_data['Sneaker Name'].apply(lambda x : 1 if 'Black' in x.split(\"-\") else 0)\n",
    "\n",
    "#Creating the airmax90 variable\n",
    "stockx_data[\"airmax90\"] = stockx_data['Sneaker Name'].apply(lambda x : 1 if '90' in x.split(\"-\") else 0)\n",
    "\n",
    "#Creating the airmax97 variable\n",
    "stockx_data[\"airmax97\"] = stockx_data['Sneaker Name'].apply(lambda x : 1 if '97' in x.split(\"-\") else 0)\n",
    "\n",
    "#Creating the zoom variable\n",
    "stockx_data[\"zoom\"] = stockx_data['Sneaker Name'].apply(lambda x : 1 if 'Zoom' in x.split(\"-\") else 0)\n",
    "\n",
    "#Creating the presto variable\n",
    "stockx_data[\"presto\"] = stockx_data['Sneaker Name'].apply(lambda x : 1 if 'Presto' in x.split(\"-\") else 0)\n",
    "\n",
    "#Creating the airforce variable\n",
    "stockx_data[\"airforce\"] = stockx_data['Sneaker Name'].apply(lambda x : 1 if 'Force' in x.split(\"-\") else 0)\n",
    "\n",
    "#Creating the blazer variable\n",
    "stockx_data[\"blazer\"] = stockx_data['Sneaker Name'].apply(lambda x : 1 if 'Blazer' in x.split(\"-\") else 0)\n",
    "\n",
    "#Creating the vapormax variable\n",
    "stockx_data[\"vapormax\"] = stockx_data['Sneaker Name'].apply(lambda x : 1 if 'VaporMax' in x.split(\"-\") else 0)\n",
    "\n",
    "#Creating the california variable\n",
    "stockx_data[\"california\"] = stockx_data[\"Buyer Region\"].apply(lambda x : 1 if 'California' in x else 0)\n",
    "\n",
    "#Creating the new_york variable\n",
    "stockx_data[\"new_york\"] = stockx_data[\"Buyer Region\"].apply(lambda x : 1 if 'New York' in x else 0)\n",
    "\n",
    "#Creating the oregon variable\n",
    "stockx_data[\"oregon\"] = stockx_data[\"Buyer Region\"].apply(lambda x : 1 if 'Oregon' in x else 0)\n",
    "\n",
    "#Creating the florida variable\n",
    "stockx_data[\"florida\"] = stockx_data[\"Buyer Region\"].apply(lambda x : 1 if 'Florida' in x else 0)\n",
    "\n",
    "#Creating the texas variable\n",
    "stockx_data[\"texas\"] = stockx_data[\"Buyer Region\"].apply(lambda x : 1 if 'Texas' in x else 0)\n",
    "\n",
    "#Creating the other_state variable\n",
    "above5pct_states = [\"California\", \"New York\", \"Oregon\", \"Florida\", \"Texas\"]\n",
    "stockx_data[\"other_state\"] = pd.Series(list(map(int,~stockx_data[\"Buyer Region\"].isin(above5pct_states))))\n",
    "\n",
    "#Creating the brand2 variable\n",
    "stockx_data[\"brand2\"] = stockx_data[\"Brand\"].apply(lambda x : 1 if 'Yeezy' in x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling\n",
    "------------\n",
    "In this section, we are going to do some predictive modeling on **price_ratio**. After doing some data processing, we will attempt some multiple linear regression models to predict **price_ratio**. After evaluating these linear models for performance and validity, we will move on to random forest regression models and compare their performances. All regression models in this notebook will be compared using mean absolute percent error, or MAPE. This metric can be interpreted as follows: \"On average, the model outputs predictions that are XX.X% off from the true value.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since **date_diff** is a direct linear combination of **Order Date** and **Release Date**, we will drop these variables before splitting the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of the date columns\n",
    "stockx_data = stockx_data.drop(['Order Date','Release Date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get rid of **Brand**, since **brand2** is a numerical version of the same information. We also get rid of **Sneaker Name** because we have extracted most of the information from that column in the form of several binary indicator variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of the original Brand and Sneaker Name columns\n",
    "stockx_data = stockx_data.drop(['Brand','Sneaker Name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to **date_diff**, we drop **Sale Price** and **Retail Price** because they are explained by our target variable, **price_ratio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of the Sale Price and Retail Price\n",
    "stockx_data = stockx_data.drop(['Sale Price', 'Retail Price'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get rid of **Buyer Region** because we have several binary indicator variables in its place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of Buyer Region\n",
    "stockx_data = stockx_data.drop(['Buyer Region'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our model-ready dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shoe Size</th>\n",
       "      <th>date_diff</th>\n",
       "      <th>price_ratio</th>\n",
       "      <th>jordan</th>\n",
       "      <th>V2</th>\n",
       "      <th>blackcol</th>\n",
       "      <th>airmax90</th>\n",
       "      <th>airmax97</th>\n",
       "      <th>zoom</th>\n",
       "      <th>presto</th>\n",
       "      <th>airforce</th>\n",
       "      <th>blazer</th>\n",
       "      <th>vapormax</th>\n",
       "      <th>california</th>\n",
       "      <th>new_york</th>\n",
       "      <th>oregon</th>\n",
       "      <th>florida</th>\n",
       "      <th>texas</th>\n",
       "      <th>other_state</th>\n",
       "      <th>brand2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>4.986364</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>3.113636</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>3.136364</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.5</td>\n",
       "      <td>282.0</td>\n",
       "      <td>4.886364</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>3.763636</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Shoe Size  date_diff  price_ratio  jordan  V2  blackcol  airmax90  \\\n",
       "0       11.0      342.0     4.986364       0   1         0         0   \n",
       "1       11.0      282.0     3.113636       0   1         1         0   \n",
       "2       11.0      282.0     3.136364       0   1         1         0   \n",
       "3       11.5      282.0     4.886364       0   1         1         0   \n",
       "4       11.0      202.0     3.763636       0   1         1         0   \n",
       "\n",
       "   airmax97  zoom  presto  airforce  blazer  vapormax  california  new_york  \\\n",
       "0         0     0       0         0       0         0           1         0   \n",
       "1         0     0       0         0       0         0           1         0   \n",
       "2         0     0       0         0       0         0           1         0   \n",
       "3         0     0       0         0       0         0           0         0   \n",
       "4         0     0       0         0       0         0           0         0   \n",
       "\n",
       "   oregon  florida  texas  other_state  brand2  \n",
       "0       0        0      0            0       1  \n",
       "1       0        0      0            0       1  \n",
       "2       0        0      0            0       1  \n",
       "3       0        0      0            1       1  \n",
       "4       0        0      0            1       1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stockx_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to split the dataset into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create objects x and y to separate predictors from the target. \n",
    "x = stockx_data.drop(\"price_ratio\", axis=1)\n",
    "y = stockx_data[\"price_ratio\"]\n",
    "\n",
    "#Run train_test_split to create 4 different data objects with train/test preds ana train/test targets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we will look at is multiple linear regression. One of the assumptions of this model is that predictors cannot be correlated to one another (multi-collinear). To address this, we look at the pearson correlation matrix for the dataset. Here we remove **brand2** because it is highly correlated with **V2**. Additionally, we remove **other_state** because it is directly explained by the **california**, **new_york**, **oregon**, **florida**, and **texas** variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shoe Size</th>\n",
       "      <th>date_diff</th>\n",
       "      <th>price_ratio</th>\n",
       "      <th>jordan</th>\n",
       "      <th>V2</th>\n",
       "      <th>blackcol</th>\n",
       "      <th>airmax90</th>\n",
       "      <th>airmax97</th>\n",
       "      <th>zoom</th>\n",
       "      <th>presto</th>\n",
       "      <th>airforce</th>\n",
       "      <th>blazer</th>\n",
       "      <th>vapormax</th>\n",
       "      <th>california</th>\n",
       "      <th>new_york</th>\n",
       "      <th>oregon</th>\n",
       "      <th>florida</th>\n",
       "      <th>texas</th>\n",
       "      <th>other_state</th>\n",
       "      <th>brand2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Shoe Size</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017450</td>\n",
       "      <td>0.082967</td>\n",
       "      <td>0.015092</td>\n",
       "      <td>-0.075669</td>\n",
       "      <td>0.040780</td>\n",
       "      <td>0.038257</td>\n",
       "      <td>0.036167</td>\n",
       "      <td>0.032316</td>\n",
       "      <td>0.017453</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0.017474</td>\n",
       "      <td>0.015025</td>\n",
       "      <td>-0.033256</td>\n",
       "      <td>-0.052401</td>\n",
       "      <td>-0.145723</td>\n",
       "      <td>0.038275</td>\n",
       "      <td>0.043572</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>-0.073214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_diff</th>\n",
       "      <td>0.017450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.134612</td>\n",
       "      <td>-0.134647</td>\n",
       "      <td>0.297042</td>\n",
       "      <td>-0.008032</td>\n",
       "      <td>-0.083015</td>\n",
       "      <td>-0.057282</td>\n",
       "      <td>-0.118693</td>\n",
       "      <td>-0.111752</td>\n",
       "      <td>-0.102679</td>\n",
       "      <td>-0.111591</td>\n",
       "      <td>-0.076988</td>\n",
       "      <td>-0.023272</td>\n",
       "      <td>0.014921</td>\n",
       "      <td>-0.067624</td>\n",
       "      <td>0.013591</td>\n",
       "      <td>0.017382</td>\n",
       "      <td>0.028702</td>\n",
       "      <td>0.332088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_ratio</th>\n",
       "      <td>0.082967</td>\n",
       "      <td>-0.134612</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.513948</td>\n",
       "      <td>-0.667221</td>\n",
       "      <td>0.210440</td>\n",
       "      <td>0.139523</td>\n",
       "      <td>0.122782</td>\n",
       "      <td>-0.058936</td>\n",
       "      <td>0.352584</td>\n",
       "      <td>0.079143</td>\n",
       "      <td>0.307107</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.061685</td>\n",
       "      <td>-0.007919</td>\n",
       "      <td>0.028950</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>-0.028866</td>\n",
       "      <td>-0.045644</td>\n",
       "      <td>-0.650173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jordan</th>\n",
       "      <td>0.015092</td>\n",
       "      <td>-0.134647</td>\n",
       "      <td>0.513948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.391907</td>\n",
       "      <td>-0.080017</td>\n",
       "      <td>-0.035130</td>\n",
       "      <td>-0.029232</td>\n",
       "      <td>-0.052261</td>\n",
       "      <td>-0.052551</td>\n",
       "      <td>-0.039284</td>\n",
       "      <td>-0.047697</td>\n",
       "      <td>-0.046362</td>\n",
       "      <td>0.020419</td>\n",
       "      <td>-0.011476</td>\n",
       "      <td>0.063604</td>\n",
       "      <td>-0.010728</td>\n",
       "      <td>-0.029019</td>\n",
       "      <td>-0.022750</td>\n",
       "      <td>-0.396354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>-0.075669</td>\n",
       "      <td>0.297042</td>\n",
       "      <td>-0.667221</td>\n",
       "      <td>-0.391907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.343572</td>\n",
       "      <td>-0.227540</td>\n",
       "      <td>-0.189339</td>\n",
       "      <td>-0.338495</td>\n",
       "      <td>-0.340376</td>\n",
       "      <td>-0.254445</td>\n",
       "      <td>-0.308933</td>\n",
       "      <td>-0.300288</td>\n",
       "      <td>-0.050426</td>\n",
       "      <td>0.009104</td>\n",
       "      <td>-0.009697</td>\n",
       "      <td>-0.011824</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>0.034503</td>\n",
       "      <td>0.988782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackcol</th>\n",
       "      <td>0.040780</td>\n",
       "      <td>-0.008032</td>\n",
       "      <td>0.210440</td>\n",
       "      <td>-0.080017</td>\n",
       "      <td>-0.343572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.128980</td>\n",
       "      <td>0.045215</td>\n",
       "      <td>0.243237</td>\n",
       "      <td>0.244106</td>\n",
       "      <td>0.157696</td>\n",
       "      <td>-0.063076</td>\n",
       "      <td>0.184024</td>\n",
       "      <td>0.034298</td>\n",
       "      <td>-0.002500</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>-0.011753</td>\n",
       "      <td>-0.022306</td>\n",
       "      <td>-0.332112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airmax90</th>\n",
       "      <td>0.038257</td>\n",
       "      <td>-0.083015</td>\n",
       "      <td>0.139523</td>\n",
       "      <td>-0.035130</td>\n",
       "      <td>-0.227540</td>\n",
       "      <td>0.128980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.016972</td>\n",
       "      <td>-0.030342</td>\n",
       "      <td>-0.030511</td>\n",
       "      <td>-0.022808</td>\n",
       "      <td>-0.027693</td>\n",
       "      <td>-0.026918</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>-0.008527</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>-0.009714</td>\n",
       "      <td>-0.230121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airmax97</th>\n",
       "      <td>0.036167</td>\n",
       "      <td>-0.057282</td>\n",
       "      <td>0.122782</td>\n",
       "      <td>-0.029232</td>\n",
       "      <td>-0.189339</td>\n",
       "      <td>0.045215</td>\n",
       "      <td>-0.016972</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025248</td>\n",
       "      <td>-0.025389</td>\n",
       "      <td>-0.018979</td>\n",
       "      <td>-0.023043</td>\n",
       "      <td>-0.022399</td>\n",
       "      <td>0.024967</td>\n",
       "      <td>-0.002788</td>\n",
       "      <td>-0.015055</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>-0.000301</td>\n",
       "      <td>-0.010113</td>\n",
       "      <td>-0.191487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom</th>\n",
       "      <td>0.032316</td>\n",
       "      <td>-0.118693</td>\n",
       "      <td>-0.058936</td>\n",
       "      <td>-0.052261</td>\n",
       "      <td>-0.338495</td>\n",
       "      <td>0.243237</td>\n",
       "      <td>-0.030342</td>\n",
       "      <td>-0.025248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.045389</td>\n",
       "      <td>-0.033930</td>\n",
       "      <td>-0.041196</td>\n",
       "      <td>-0.040044</td>\n",
       "      <td>0.005273</td>\n",
       "      <td>-0.006319</td>\n",
       "      <td>-0.036907</td>\n",
       "      <td>0.020265</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>0.005971</td>\n",
       "      <td>-0.342336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presto</th>\n",
       "      <td>0.017453</td>\n",
       "      <td>-0.111752</td>\n",
       "      <td>0.352584</td>\n",
       "      <td>-0.052551</td>\n",
       "      <td>-0.340376</td>\n",
       "      <td>0.244106</td>\n",
       "      <td>-0.030511</td>\n",
       "      <td>-0.025389</td>\n",
       "      <td>-0.045389</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034119</td>\n",
       "      <td>-0.041425</td>\n",
       "      <td>-0.040266</td>\n",
       "      <td>0.018024</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>-0.008011</td>\n",
       "      <td>-0.016277</td>\n",
       "      <td>-0.344238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airforce</th>\n",
       "      <td>0.009827</td>\n",
       "      <td>-0.102679</td>\n",
       "      <td>0.079143</td>\n",
       "      <td>-0.039284</td>\n",
       "      <td>-0.254445</td>\n",
       "      <td>0.157696</td>\n",
       "      <td>-0.022808</td>\n",
       "      <td>-0.018979</td>\n",
       "      <td>-0.033930</td>\n",
       "      <td>-0.034119</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.030967</td>\n",
       "      <td>-0.030101</td>\n",
       "      <td>0.013784</td>\n",
       "      <td>-0.006570</td>\n",
       "      <td>0.027006</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>-0.005501</td>\n",
       "      <td>-0.019926</td>\n",
       "      <td>-0.257332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blazer</th>\n",
       "      <td>0.017474</td>\n",
       "      <td>-0.111591</td>\n",
       "      <td>0.307107</td>\n",
       "      <td>-0.047697</td>\n",
       "      <td>-0.308933</td>\n",
       "      <td>-0.063076</td>\n",
       "      <td>-0.027693</td>\n",
       "      <td>-0.023043</td>\n",
       "      <td>-0.041196</td>\n",
       "      <td>-0.041425</td>\n",
       "      <td>-0.030967</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.036546</td>\n",
       "      <td>0.020984</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>-0.001104</td>\n",
       "      <td>-0.010907</td>\n",
       "      <td>-0.014853</td>\n",
       "      <td>-0.312438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vapormax</th>\n",
       "      <td>0.015025</td>\n",
       "      <td>-0.076988</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>-0.046362</td>\n",
       "      <td>-0.300288</td>\n",
       "      <td>0.184024</td>\n",
       "      <td>-0.026918</td>\n",
       "      <td>-0.022399</td>\n",
       "      <td>-0.040044</td>\n",
       "      <td>-0.040266</td>\n",
       "      <td>-0.030101</td>\n",
       "      <td>-0.036546</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012970</td>\n",
       "      <td>0.007710</td>\n",
       "      <td>-0.040139</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>-0.001537</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>-0.303695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>california</th>\n",
       "      <td>-0.033256</td>\n",
       "      <td>-0.023272</td>\n",
       "      <td>0.061685</td>\n",
       "      <td>0.020419</td>\n",
       "      <td>-0.050426</td>\n",
       "      <td>0.034298</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.024967</td>\n",
       "      <td>0.005273</td>\n",
       "      <td>0.018024</td>\n",
       "      <td>0.013784</td>\n",
       "      <td>0.020984</td>\n",
       "      <td>0.012970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.218047</td>\n",
       "      <td>-0.141355</td>\n",
       "      <td>-0.127887</td>\n",
       "      <td>-0.122443</td>\n",
       "      <td>-0.435772</td>\n",
       "      <td>-0.048364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_york</th>\n",
       "      <td>-0.052401</td>\n",
       "      <td>0.014921</td>\n",
       "      <td>-0.007919</td>\n",
       "      <td>-0.011476</td>\n",
       "      <td>0.009104</td>\n",
       "      <td>-0.002500</td>\n",
       "      <td>-0.008527</td>\n",
       "      <td>-0.002788</td>\n",
       "      <td>-0.006319</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>-0.006570</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.007710</td>\n",
       "      <td>-0.218047</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.128403</td>\n",
       "      <td>-0.116169</td>\n",
       "      <td>-0.111224</td>\n",
       "      <td>-0.395843</td>\n",
       "      <td>0.010398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oregon</th>\n",
       "      <td>-0.145723</td>\n",
       "      <td>-0.067624</td>\n",
       "      <td>0.028950</td>\n",
       "      <td>0.063604</td>\n",
       "      <td>-0.009697</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>-0.015055</td>\n",
       "      <td>-0.036907</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>0.027006</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>-0.040139</td>\n",
       "      <td>-0.141355</td>\n",
       "      <td>-0.128403</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.075309</td>\n",
       "      <td>-0.072104</td>\n",
       "      <td>-0.256616</td>\n",
       "      <td>-0.012509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>florida</th>\n",
       "      <td>0.038275</td>\n",
       "      <td>0.013591</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>-0.010728</td>\n",
       "      <td>-0.011824</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.020265</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>-0.001104</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>-0.127887</td>\n",
       "      <td>-0.116169</td>\n",
       "      <td>-0.075309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.065234</td>\n",
       "      <td>-0.232166</td>\n",
       "      <td>-0.010880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texas</th>\n",
       "      <td>0.043572</td>\n",
       "      <td>0.017382</td>\n",
       "      <td>-0.028866</td>\n",
       "      <td>-0.029019</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>-0.011753</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>-0.000301</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>-0.008011</td>\n",
       "      <td>-0.005501</td>\n",
       "      <td>-0.010907</td>\n",
       "      <td>-0.001537</td>\n",
       "      <td>-0.122443</td>\n",
       "      <td>-0.111224</td>\n",
       "      <td>-0.072104</td>\n",
       "      <td>-0.065234</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.222284</td>\n",
       "      <td>0.020209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_state</th>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.028702</td>\n",
       "      <td>-0.045644</td>\n",
       "      <td>-0.022750</td>\n",
       "      <td>0.034503</td>\n",
       "      <td>-0.022306</td>\n",
       "      <td>-0.009714</td>\n",
       "      <td>-0.010113</td>\n",
       "      <td>0.005971</td>\n",
       "      <td>-0.016277</td>\n",
       "      <td>-0.019926</td>\n",
       "      <td>-0.014853</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>-0.435772</td>\n",
       "      <td>-0.395843</td>\n",
       "      <td>-0.256616</td>\n",
       "      <td>-0.232166</td>\n",
       "      <td>-0.222284</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand2</th>\n",
       "      <td>-0.073214</td>\n",
       "      <td>0.332088</td>\n",
       "      <td>-0.650173</td>\n",
       "      <td>-0.396354</td>\n",
       "      <td>0.988782</td>\n",
       "      <td>-0.332112</td>\n",
       "      <td>-0.230121</td>\n",
       "      <td>-0.191487</td>\n",
       "      <td>-0.342336</td>\n",
       "      <td>-0.344238</td>\n",
       "      <td>-0.257332</td>\n",
       "      <td>-0.312438</td>\n",
       "      <td>-0.303695</td>\n",
       "      <td>-0.048364</td>\n",
       "      <td>0.010398</td>\n",
       "      <td>-0.012509</td>\n",
       "      <td>-0.010880</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>0.033192</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Shoe Size  date_diff  price_ratio    jordan        V2  blackcol  \\\n",
       "Shoe Size     1.000000   0.017450     0.082967  0.015092 -0.075669  0.040780   \n",
       "date_diff     0.017450   1.000000    -0.134612 -0.134647  0.297042 -0.008032   \n",
       "price_ratio   0.082967  -0.134612     1.000000  0.513948 -0.667221  0.210440   \n",
       "jordan        0.015092  -0.134647     0.513948  1.000000 -0.391907 -0.080017   \n",
       "V2           -0.075669   0.297042    -0.667221 -0.391907  1.000000 -0.343572   \n",
       "blackcol      0.040780  -0.008032     0.210440 -0.080017 -0.343572  1.000000   \n",
       "airmax90      0.038257  -0.083015     0.139523 -0.035130 -0.227540  0.128980   \n",
       "airmax97      0.036167  -0.057282     0.122782 -0.029232 -0.189339  0.045215   \n",
       "zoom          0.032316  -0.118693    -0.058936 -0.052261 -0.338495  0.243237   \n",
       "presto        0.017453  -0.111752     0.352584 -0.052551 -0.340376  0.244106   \n",
       "airforce      0.009827  -0.102679     0.079143 -0.039284 -0.254445  0.157696   \n",
       "blazer        0.017474  -0.111591     0.307107 -0.047697 -0.308933 -0.063076   \n",
       "vapormax      0.015025  -0.076988     0.042200 -0.046362 -0.300288  0.184024   \n",
       "california   -0.033256  -0.023272     0.061685  0.020419 -0.050426  0.034298   \n",
       "new_york     -0.052401   0.014921    -0.007919 -0.011476  0.009104 -0.002500   \n",
       "oregon       -0.145723  -0.067624     0.028950  0.063604 -0.009697  0.000382   \n",
       "florida       0.038275   0.013591     0.001291 -0.010728 -0.011824  0.004574   \n",
       "texas         0.043572   0.017382    -0.028866 -0.029019  0.020747 -0.011753   \n",
       "other_state   0.104341   0.028702    -0.045644 -0.022750  0.034503 -0.022306   \n",
       "brand2       -0.073214   0.332088    -0.650173 -0.396354  0.988782 -0.332112   \n",
       "\n",
       "             airmax90  airmax97      zoom    presto  airforce    blazer  \\\n",
       "Shoe Size    0.038257  0.036167  0.032316  0.017453  0.009827  0.017474   \n",
       "date_diff   -0.083015 -0.057282 -0.118693 -0.111752 -0.102679 -0.111591   \n",
       "price_ratio  0.139523  0.122782 -0.058936  0.352584  0.079143  0.307107   \n",
       "jordan      -0.035130 -0.029232 -0.052261 -0.052551 -0.039284 -0.047697   \n",
       "V2          -0.227540 -0.189339 -0.338495 -0.340376 -0.254445 -0.308933   \n",
       "blackcol     0.128980  0.045215  0.243237  0.244106  0.157696 -0.063076   \n",
       "airmax90     1.000000 -0.016972 -0.030342 -0.030511 -0.022808 -0.027693   \n",
       "airmax97    -0.016972  1.000000 -0.025248 -0.025389 -0.018979 -0.023043   \n",
       "zoom        -0.030342 -0.025248  1.000000 -0.045389 -0.033930 -0.041196   \n",
       "presto      -0.030511 -0.025389 -0.045389  1.000000 -0.034119 -0.041425   \n",
       "airforce    -0.022808 -0.018979 -0.033930 -0.034119  1.000000 -0.030967   \n",
       "blazer      -0.027693 -0.023043 -0.041196 -0.041425 -0.030967  1.000000   \n",
       "vapormax    -0.026918 -0.022399 -0.040044 -0.040266 -0.030101 -0.036546   \n",
       "california   0.001490  0.024967  0.005273  0.018024  0.013784  0.020984   \n",
       "new_york    -0.008527 -0.002788 -0.006319  0.000883 -0.006570  0.002335   \n",
       "oregon       0.022400 -0.015055 -0.036907 -0.000601  0.027006  0.003954   \n",
       "florida      0.000746  0.001120  0.020265  0.010959  0.004054 -0.001104   \n",
       "texas        0.005332 -0.000301  0.009252 -0.008011 -0.005501 -0.010907   \n",
       "other_state -0.009714 -0.010113  0.005971 -0.016277 -0.019926 -0.014853   \n",
       "brand2      -0.230121 -0.191487 -0.342336 -0.344238 -0.257332 -0.312438   \n",
       "\n",
       "             vapormax  california  new_york    oregon   florida     texas  \\\n",
       "Shoe Size    0.015025   -0.033256 -0.052401 -0.145723  0.038275  0.043572   \n",
       "date_diff   -0.076988   -0.023272  0.014921 -0.067624  0.013591  0.017382   \n",
       "price_ratio  0.042200    0.061685 -0.007919  0.028950  0.001291 -0.028866   \n",
       "jordan      -0.046362    0.020419 -0.011476  0.063604 -0.010728 -0.029019   \n",
       "V2          -0.300288   -0.050426  0.009104 -0.009697 -0.011824  0.020747   \n",
       "blackcol     0.184024    0.034298 -0.002500  0.000382  0.004574 -0.011753   \n",
       "airmax90    -0.026918    0.001490 -0.008527  0.022400  0.000746  0.005332   \n",
       "airmax97    -0.022399    0.024967 -0.002788 -0.015055  0.001120 -0.000301   \n",
       "zoom        -0.040044    0.005273 -0.006319 -0.036907  0.020265  0.009252   \n",
       "presto      -0.040266    0.018024  0.000883 -0.000601  0.010959 -0.008011   \n",
       "airforce    -0.030101    0.013784 -0.006570  0.027006  0.004054 -0.005501   \n",
       "blazer      -0.036546    0.020984  0.002335  0.003954 -0.001104 -0.010907   \n",
       "vapormax     1.000000    0.012970  0.007710 -0.040139  0.001860 -0.001537   \n",
       "california   0.012970    1.000000 -0.218047 -0.141355 -0.127887 -0.122443   \n",
       "new_york     0.007710   -0.218047  1.000000 -0.128403 -0.116169 -0.111224   \n",
       "oregon      -0.040139   -0.141355 -0.128403  1.000000 -0.075309 -0.072104   \n",
       "florida      0.001860   -0.127887 -0.116169 -0.075309  1.000000 -0.065234   \n",
       "texas       -0.001537   -0.122443 -0.111224 -0.072104 -0.065234  1.000000   \n",
       "other_state  0.005254   -0.435772 -0.395843 -0.256616 -0.232166 -0.222284   \n",
       "brand2      -0.303695   -0.048364  0.010398 -0.012509 -0.010880  0.020209   \n",
       "\n",
       "             other_state    brand2  \n",
       "Shoe Size       0.104341 -0.073214  \n",
       "date_diff       0.028702  0.332088  \n",
       "price_ratio    -0.045644 -0.650173  \n",
       "jordan         -0.022750 -0.396354  \n",
       "V2              0.034503  0.988782  \n",
       "blackcol       -0.022306 -0.332112  \n",
       "airmax90       -0.009714 -0.230121  \n",
       "airmax97       -0.010113 -0.191487  \n",
       "zoom            0.005971 -0.342336  \n",
       "presto         -0.016277 -0.344238  \n",
       "airforce       -0.019926 -0.257332  \n",
       "blazer         -0.014853 -0.312438  \n",
       "vapormax        0.005254 -0.303695  \n",
       "california     -0.435772 -0.048364  \n",
       "new_york       -0.395843  0.010398  \n",
       "oregon         -0.256616 -0.012509  \n",
       "florida        -0.232166 -0.010880  \n",
       "texas          -0.222284  0.020209  \n",
       "other_state     1.000000  0.033192  \n",
       "brand2          0.033192  1.000000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at the correlation values for each predictor relative to each other and the target\n",
    "stockx_data.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating copies of the train/test datasets without brand2 or other_state\n",
    "x_train_m1 = x_train.copy()\n",
    "x_train_m1 = x_train_m1.drop([\"brand2\",\"other_state\"], axis=1)\n",
    "x_test_m1 = x_test.copy()\n",
    "x_test_m1 = x_test_m1.drop([\"brand2\",\"other_state\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create and fit our linear model object. This first model is predicting raw **price_ratio** values from both the Yeezy and Off-White data in the same model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a linear regression object: lm1\n",
    "lm1 = linear_model.LinearRegression()\n",
    "\n",
    "#Fitting the model with the first model's predictors (x_train_m1) and the train response\n",
    "m1 = lm1.fit(x_train_m1, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the parameter estimates, p-values, R-squared, and adjusted R-squared for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== SUMMARY ===========\n",
      "Residuals:\n",
      "     Min      1Q  Median      3Q    Max\n",
      "-15.9429 -0.2307  0.1848  0.4306  3.032\n",
      "\n",
      "\n",
      "Coefficients:\n",
      "            Estimate  Std. Error   t value   p value\n",
      "_intercept  3.214060    0.023309  137.8891  0.000000\n",
      "Shoe Size   0.023458    0.000546   42.9727  0.000000\n",
      "date_diff   0.000404    0.000011   37.6226  0.000000\n",
      "jordan      1.913341    0.022686   84.3393  0.000000\n",
      "V2         -1.949216    0.015933 -122.3398  0.000000\n",
      "blackcol    0.538586    0.011836   45.5038  0.000000\n",
      "airmax90    0.025745    0.030435    0.8459  0.397609\n",
      "airmax97    0.167182    0.034049    4.9100  0.000001\n",
      "zoom       -1.894389    0.024318  -77.8993  0.000000\n",
      "presto      0.998485    0.024160   41.3280  0.000000\n",
      "airforce   -0.701340    0.028449  -24.6523  0.000000\n",
      "blazer      1.143490    0.025426   44.9735  0.000000\n",
      "vapormax   -1.126185    0.025734  -43.7619  0.000000\n",
      "california  0.101647    0.008280   12.2766  0.000000\n",
      "new_york    0.036453    0.008802    4.1415  0.000035\n",
      "oregon      0.029056    0.012254    2.3711  0.017737\n",
      "florida     0.021535    0.013210    1.6302  0.103070\n",
      "texas      -0.006748    0.013633   -0.4950  0.620610\n",
      "---\n",
      "R-squared:  0.65331,    Adjusted R-squared:  0.65324\n",
      "F-statistic: 8862.05 on 17 features\n"
     ]
    }
   ],
   "source": [
    "#Get the summary table with p-values for each predictor. \n",
    "print(\"\\n=========== SUMMARY ===========\")\n",
    "xlabels = x_train_m1.columns\n",
    "stats_reg.summary(lm1, x_train_m1, y_train, xlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict using the testing predictor data and calculate the residuals. Based on the following QQ-plot for these residuals, this model breaks the assumption of normally distributed residuals. So while the parameter estimates are still valid, this model should not be used to predict on new data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Getting test predictions\n",
    "test_preds_m1 = lm1.predict(x_test_m1)\n",
    "\n",
    "#Calculating residuals\n",
    "residuals_m1 = test_preds_m1 - y_test\n",
    "\n",
    "#Making QQ-plot of residuals\n",
    "sm.qqplot(residuals_m1,line=\"s\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the mean absolute percent error of the model (MAPE) for comparison to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.486227544958503"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the MAPE\n",
    "lm1_mape = np.mean(100 * abs((residuals_m1/y_test)))\n",
    "lm1_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression - Log Transformed Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the residual QQ-plot from the previous model, the assumptions of linear regression were not met. This issue could be happening because **price_ratio** is a bounded quantity. As I mentioned earlier, it is impossible to have a **price_ratio** less than or equal to 0. Additionally, it is extremely rare that a sneaker has a price ratio less than 1 (557 transactions of 99956 observations). But **price_ratio** has no upper bound. Because of this asymmetrical bound, the model predicting raw **price_ratio** values gets wonky around 1. By taking the natural log of **price_ratio** we convert the target into an unbounded logarithmic scale, which improves the symmetry and spread of the target distribution. Looking at the overlaid histogram below, it is clear that the frequencies for individual values goes down quite significantly after log transformatio and that the spread improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the overlaid histogram of the log-transformed target and regular target\n",
    "plt.hist(np.log(stockx_data[\"price_ratio\"]),bins=100, alpha = 0.5);\n",
    "plt.hist(stockx_data[\"price_ratio\"], bins = 100, alpha = 0.2);\n",
    "plt.legend([\"log(price_ratio)\", \"price_ratio\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time when making our copies of the data, we must log transform the training labels (y values). We do not touch the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating copies of the train/test datasets without brand2 in it AND changing the y test/train to be log transformed. \n",
    "x_train_m2 = x_train.copy()\n",
    "x_train_m2 = x_train_m2.drop([\"brand2\",\"other_state\"], axis=1)\n",
    "x_test_m2 = x_test.copy()\n",
    "x_test_m2 = x_test_m2.drop([\"brand2\", \"other_state\"], axis=1)\n",
    "\n",
    "y_train_m2 = y_train.copy()\n",
    "y_train_m2 = np.log(y_train_m2)\n",
    "y_test_m2 = y_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create and fit our linear model object. This first model is predicting log transformed **price_ratio** values from both the Yeezy and Off-White data in the same model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the linear model object: lm2\n",
    "lm2 = linear_model.LinearRegression()\n",
    "\n",
    "#Fitting the linear model: m2\n",
    "m2 = lm2.fit(x_train_m2, y_train_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the parameter estimates, p-values, R-squared, and adjusted R-squared for the log transformed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== SUMMARY ===========\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max\n",
      "-1.8402 -0.1542  0.0599  0.2146  0.7836\n",
      "\n",
      "\n",
      "Coefficients:\n",
      "            Estimate  Std. Error   t value   p value\n",
      "_intercept  1.061363    0.007462  142.2447  0.000000\n",
      "Shoe Size   0.009461    0.000175   54.1387  0.000000\n",
      "date_diff   0.000015    0.000003    4.4379  0.000009\n",
      "jordan      0.447876    0.007262   61.6726  0.000000\n",
      "V2         -0.743512    0.005100 -145.7781  0.000000\n",
      "blackcol    0.287930    0.003789   75.9935  0.000000\n",
      "airmax90    0.020843    0.009743    2.1394  0.032410\n",
      "airmax97    0.080232    0.010900    7.3610  0.000000\n",
      "zoom       -0.720901    0.007785  -92.6054  0.000000\n",
      "presto      0.225902    0.007734   29.2093  0.000000\n",
      "airforce   -0.229710    0.009107  -25.2235  0.000000\n",
      "blazer      0.346489    0.008139   42.5707  0.000000\n",
      "vapormax   -0.343320    0.008238  -41.6756  0.000000\n",
      "california  0.036606    0.002650   13.8112  0.000000\n",
      "new_york    0.013510    0.002818    4.7950  0.000002\n",
      "oregon      0.034465    0.003923    8.7860  0.000000\n",
      "florida     0.006789    0.004229    1.6055  0.108394\n",
      "texas      -0.007176    0.004364   -1.6443  0.100111\n",
      "---\n",
      "R-squared:  0.68851,    Adjusted R-squared:  0.68844\n",
      "F-statistic: 10394.62 on 17 features\n"
     ]
    }
   ],
   "source": [
    "#Printing summary\n",
    "print(\"\\n=========== SUMMARY ===========\")\n",
    "xlabels = x_train_m2.columns\n",
    "stats_reg.summary(lm2, x_train_m2, y_train_m2, xlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict using the testing predictor data and calculate the residuals. Based on the following QQ-plot for these residuals, this model still breaks the assumption of normally distributed residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Getting test predictions\n",
    "test_preds_m2 = lm2.predict(x_test_m2)\n",
    "\n",
    "#Calculating residuals\n",
    "residuals_m2 = np.exp(test_preds_m2) - y_test_m2\n",
    "\n",
    "#Making QQ-plot of the residuals\n",
    "sm.qqplot(residuals_m2,line=\"s\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the MAPE of the log transformed model. This is a slight improvement (lower MAPE) compared to the previous model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.070558390907753"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the MAPE: On average, the log transformed linear model is 22.03% off in predicting price\n",
    "#ratio. \n",
    "lmlog_mape = np.mean(100 * abs(residuals_m2/y_test_m2))\n",
    "lmlog_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results and looking back on the distributions of **price_ratio** for Yeezys and Off-Whites, it makes sense to pursue separate linear regression models for each brand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression: Yeezys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dataset this time around is not quite as simple as dropping unwanted columns. First we drop **other_state** for collinearity concerns. Then we subset the training and testing data for rows in which **brand2** = 1 (Yeezy). After that, we drop **jordan**, **airmax90**, **airmax97**, **zoom**, **presto**, **airforce**, **blazer**, and **vapormax** because those columns exclusively apply to Off-Whites. After that, we drop **brand2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 3rd copy of the x_train and x_test\n",
    "x_train_m3 = x_train.copy()\n",
    "x_train_m3 = x_train_m3.drop([\"other_state\"], axis=1)\n",
    "x_test_m3 = x_test.copy()\n",
    "x_test_m3 = x_test_m3.drop([\"other_state\"], axis=1)\n",
    "\n",
    "#Subset only rows with brand2 == 1 (Yeezy). Note that y_train_yeezy is log transformed\n",
    "x_train_yeezy = x_train_m3.loc[x_train_m3[\"brand2\"] == 1,]\n",
    "x_test_yeezy = x_test_m3.loc[x_test_m3[\"brand2\"] == 1,]\n",
    "y_train_yeezy = np.log(y_train.loc[x_train_m3[\"brand2\"] == 1,])\n",
    "y_test_yeezy = y_test.loc[x_test_m3[\"brand2\"] == 1,]\n",
    "\n",
    "#Drop: jordan, airmax90, airmax97, zoom, presto, airforce, blazer, vapormax, brand2. \n",
    "x_train_yeezy = x_train_yeezy.drop([\"jordan\", \"airmax90\", \"airmax97\", \"zoom\", \\\n",
    "                                    \"presto\", \"airforce\",\"blazer\", \"vapormax\",\"brand2\"],axis = 1)\n",
    "x_test_yeezy = x_test_yeezy.drop([\"jordan\", \"airmax90\", \"airmax97\", \"zoom\",\\\n",
    "                   \"presto\", \"airforce\",\"blazer\", \"vapormax\",\"brand2\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create and fit our linear model object. This third model is predicting log transformed **price_ratio** values from only the Yeezy shoes in the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a linear model object: lm_yeezy\n",
    "lm_yeezy = linear_model.LinearRegression()\n",
    "\n",
    "#Training the Yeezy model using the Yeezy train/target\n",
    "m_yeezy = lm_yeezy.fit(x_train_yeezy, y_train_yeezy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the parameter estimates, p-values, R-squared, and adjusted R-squared for the log transformed Yeezy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== SUMMARY ===========\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max\n",
      "-1.7734 -0.1544  0.0263  0.2271  0.8295\n",
      "\n",
      "\n",
      "Coefficients:\n",
      "            Estimate  Std. Error   t value   p value\n",
      "_intercept  1.295154    0.012556  103.1533  0.000000\n",
      "Shoe Size   0.007168    0.000211   33.9452  0.000000\n",
      "date_diff  -0.000156    0.000003  -47.6076  0.000000\n",
      "V2         -0.931920    0.011314  -82.3673  0.000000\n",
      "blackcol    0.824199    0.006238  132.1305  0.000000\n",
      "california  0.023755    0.002982    7.9652  0.000000\n",
      "new_york    0.008527    0.003084    2.7646  0.005701\n",
      "oregon      0.045073    0.004337   10.3924  0.000000\n",
      "florida     0.007669    0.004713    1.6273  0.103676\n",
      "texas      -0.010246    0.004688   -2.1857  0.028843\n",
      "---\n",
      "R-squared:  0.30724,    Adjusted R-squared:  0.30713\n",
      "F-statistic: 2844.34 on 9 features\n"
     ]
    }
   ],
   "source": [
    "#Printing summary\n",
    "print(\"\\n=========== SUMMARY ===========\")\n",
    "xlabels = x_train_yeezy.columns\n",
    "stats_reg.summary(lm_yeezy, x_train_yeezy, y_train_yeezy, xlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict and view the QQ-plot of the residuals. This model also fails to meet the normality of residuals requirement for linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Getting the test predictions\n",
    "test_preds_yeezy = lm_yeezy.predict(x_test_yeezy)\n",
    "\n",
    "#Calculating the residuals. The test predictions have to be exponentiated when compared to the test set.\n",
    "residuals_yeezy = np.exp(test_preds_yeezy) - y_test_yeezy\n",
    "\n",
    "#Making the QQ-plot of the residuals\n",
    "sm.qqplot(residuals_yeezy,line=\"s\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the MAPE of the Yeezy linear model. Separating by brand seems to have slightly reduced the MAPE in the case of the Yeezys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.90089303796256"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the MAPE\n",
    "lmyeezy_mape = np.mean(100 * abs(residuals_yeezy/y_test_yeezy))\n",
    "lmyeezy_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression: Off-Whites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how the Yeezy data and labels were created, we subset the data to only rows with **brand2** = 0 (Off-White) and then get rid of **V2** (a strictly Yeezy term) and **brand2**, since it is no longer needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating yeezy and off-white training and testing sets for brand separated models\n",
    "x_train_offwhite = x_train_m3.loc[x_train_m3[\"brand2\"] == 0,]\n",
    "x_test_offwhite = x_test_m3.loc[x_test_m3[\"brand2\"] == 0,]\n",
    "\n",
    "#Taking the natural log of the training target \n",
    "y_train_offwhite = np.log(y_train.loc[x_train_m3[\"brand2\"] == 0,])\n",
    "y_test_offwhite = y_test.loc[x_test_m3[\"brand2\"] == 0,]\n",
    "\n",
    "#Drop: V2, brand2.\n",
    "x_train_offwhite = x_train_offwhite.drop([\"V2\",\"brand2\"],axis=1)\n",
    "x_test_offwhite = x_test_offwhite.drop([\"V2\",\"brand2\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create and fit our linear model object. This fourth linear model is predicting log transformed **price_ratio** values from only the Off-White shoes in the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating linear regression object for the Off-Whites\n",
    "lm_offwhite = linear_model.LinearRegression()\n",
    "\n",
    "#Training the Off-White model using the Off-White train/target\n",
    "m_offwhite = lm_offwhite.fit(x_train_offwhite, y_train_offwhite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== SUMMARY ===========\n",
      "Residuals:\n",
      "    Min    1Q  Median      3Q     Max\n",
      "-1.7974 -0.11  0.0221  0.1479  0.6967\n",
      "\n",
      "\n",
      "Coefficients:\n",
      "            Estimate  Std. Error   t value   p value\n",
      "_intercept  0.601594    0.010727   56.0818  0.000000\n",
      "Shoe Size   0.011133    0.000264   42.1456  0.000000\n",
      "date_diff   0.001363    0.000015   87.9467  0.000000\n",
      "jordan      0.820576    0.007233  113.4456  0.000000\n",
      "blackcol    0.032885    0.003877    8.4809  0.000000\n",
      "airmax90    0.493953    0.009178   53.8188  0.000000\n",
      "airmax97    0.483276    0.010069   47.9986  0.000000\n",
      "zoom       -0.237747    0.007651  -31.0759  0.000000\n",
      "presto      0.698351    0.007602   91.8649  0.000000\n",
      "airforce    0.270938    0.008682   31.2071  0.000000\n",
      "blazer      0.726895    0.007901   92.0031  0.000000\n",
      "vapormax    0.082580    0.008019   10.2984  0.000000\n",
      "california  0.030268    0.003931    7.7006  0.000000\n",
      "new_york    0.011215    0.004456    2.5168  0.011850\n",
      "oregon      0.006309    0.006058    1.0415  0.297647\n",
      "florida     0.009189    0.006371    1.4423  0.149243\n",
      "texas       0.003119    0.007259    0.4296  0.667472\n",
      "---\n",
      "R-squared:  0.73704,    Adjusted R-squared:  0.73685\n",
      "F-statistic: 3891.98 on 16 features\n"
     ]
    }
   ],
   "source": [
    "#Printing summary\n",
    "print(\"\\n=========== SUMMARY ===========\")\n",
    "xlabels = x_train_offwhite.columns\n",
    "stats_reg.summary(lm_offwhite, x_train_offwhite, y_train_offwhite, xlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict and view the QQ-plot of the residuals. Like all previous linear models, this one fails to meet the normality of residuals requirement for linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Getting test predictions\n",
    "test_preds_offwhite = lm_offwhite.predict(x_test_offwhite)\n",
    "\n",
    "#Calculating residuals\n",
    "residuals_offwhite = np.exp(test_preds_offwhite) - y_test_offwhite\n",
    "\n",
    "#Plotting QQ-plot\n",
    "sm.qqplot(residuals_offwhite,line=\"s\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the MAPE of the Off-White model. This one is the best performing model, and this is most likely due to the greater number of effective predictors in the Off-White dataset as compared to the Yeezy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.872500274511413"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the MAPE\n",
    "lmoffwhite_mape = np.mean(100 * abs(residuals_offwhite/y_test_offwhite))\n",
    "lmoffwhite_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these four failed modeling attempts with linear regression, the best course of action is to pursue a model that does not have strict assumptions like those of linear regression. Random forest model it is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "The first random forest model runs on both Yeezy and Off-White data to predict **price_ratio**.\n",
    "\n",
    "I settled on two hyperparameters to tune: number of regression trees and maximum number of features considered for each split. My process with optimizing the number of regression trees was quite simple, I just ran 4 different random forests with 10, 100, 500, and 1000 trees in them and looked at the MAPEs to determine where the point of diminishing returns was, and apply that number of trees for all subsequent models. As for the maximum number of features in a split, I created a function that can adapt to any dataset and outputs MAPEs for each possible value of max_features in the sklearn function RandomForestRegressor. I would then look for the lowest MAPE and report that as the final random forest regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make some fresh copies of the originally split data. The training set has to be further broken into training and validation in anticipation of hyperparameter tuning. Ultimately the split here is 60% training, 20% validation, and 20% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the copies of the training, testing, and validation data for the general random forest model\n",
    "x_train_m4 = x_train.copy()\n",
    "x_test_m4 = x_test.copy()\n",
    "\n",
    "y_train_m4 = y_train.copy()\n",
    "y_test_m4 = y_test.copy()\n",
    "\n",
    "\n",
    "x_train_m4_small, x_valid, y_train_m4_small, y_valid = train_test_split(x_train_m4,y_train_m4,test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I settled on two hyperparameters to tune: number of regression trees and maximum number of features considered at each split. The code below shows how I selected the number of trees. Knowing that more trees equates to more run time, I picked the tree value that seemed to mark the beginning of the point of diminished returns, which in this case was 100 trees. All random forest models use 100 trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 17.47331926011413,\n",
       " 100: 17.187102968106814,\n",
       " 500: 17.15515866882791,\n",
       " 1000: 17.157915831044022}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing 10, 100, 500, 1000 trees for their resulting MAPE values. \n",
    "trees_to_test = [10, 100, 500, 1000]\n",
    "tree_mapes = {}\n",
    "for i in trees_to_test:\n",
    "    rf_treevalid = RandomForestRegressor(random_state=np.random.seed(42), n_estimators=i)\n",
    "    rf_treevalid.fit(x_train_m4_small, y_train_m4_small)\n",
    "    rf_treevalid_preds = rf_treevalid.predict(x_valid)\n",
    "    rf_treevalid_errors = rf_treevalid_preds - y_valid\n",
    "    mape = mean(100 * abs((rf_treevalid_errors/rf_treevalid_preds)))\n",
    "    tree_mapes.update({i:mape})\n",
    "\n",
    "tree_mapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next hyperparameter I selected for tuning is the maximum features considered at every split. To do this effectively, I created a function that inputs a training data, training labels, testing data, testing labels, and number of trees. For every value from 1 to the total number of features in the data, this function will fit a random forest on the testing with 100 trees, and return the MAPE. I then pick the lowest MAPE and that corresponding number of features and that is the final random forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new function max_features_tuning to find the best value for max_features (meaning number \n",
    "#of features considered at every node) for each random forest. Trees will default to 100 because \n",
    "#that's what I determined to be the best tradeoff of accuracy vs runtime, but it can be altered.\n",
    "def max_features_tuning (xtrainset, ytrainset, xvalidset, yvalidset, trees = 100):\n",
    "    MAPE_values = {}\n",
    "    for i in range(0, xtrainset.shape[1]):\n",
    "        rf = RandomForestRegressor(random_state=np.random.seed(42), n_estimators=trees,\\\n",
    "                                   max_features=i+1)\n",
    "        rf.fit(xtrainset, ytrainset)\n",
    "        rf_preds = rf.predict(xvalidset)\n",
    "        rf_errors = rf_preds - yvalidset\n",
    "        mape = mean(100 * abs((rf_errors/rf_preds)))\n",
    "        MAPE_values.update({i+1:mape})\n",
    "    return MAPE_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output below shows each possible value of the max features parameter and the corresponding MAPE. The best value for max features for this model is 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 17.408480455320845,\n",
       " 2: 17.38573418217717,\n",
       " 3: 17.341318114640472,\n",
       " 4: 17.305922700264787,\n",
       " 5: 17.28213083880628,\n",
       " 6: 17.22375457991098,\n",
       " 7: 17.180992130584144,\n",
       " 8: 17.153795662725074,\n",
       " 9: 17.115757668900706,\n",
       " 10: 17.107120277985327,\n",
       " 11: 17.105195500687014,\n",
       " 12: 17.081304489539725,\n",
       " 13: 17.098639399282252,\n",
       " 14: 17.092345829281022,\n",
       " 15: 17.11769662828325,\n",
       " 16: 17.140016020313343,\n",
       " 17: 17.141027852619388,\n",
       " 18: 17.16826702432452,\n",
       " 19: 17.187102968106814}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking for the best max_features value using training and validation set\n",
    "max_features_tuning(xtrainset=x_train_m4_small, ytrainset=y_train_m4_small,\\\n",
    "                    xvalidset=x_valid, yvalidset = y_valid, trees = 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that trees and max features at any split have been optimized, I rerun the random forest to get the feature importances for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=12, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rerun the general Random forest with the optimal parameters  \n",
    "rf = RandomForestRegressor(random_state=np.random.seed(42), max_features=12, n_estimators=100)\n",
    "rf.fit(x_train_m4, y_train_m4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output below shows the feature importances for this random forest model. The most important variable for this random forest is **V2**, or whether or not a sneaker is a Yeezy 350 Boost V2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shoe Size ---- importance:  0.050395232310618834\n",
      "date_diff ---- importance:  0.20761292492964023\n",
      "jordan ---- importance:  0.08030481037927256\n",
      "V2 ---- importance:  0.3278791073374367\n",
      "blackcol ---- importance:  0.04533011158398275\n",
      "airmax90 ---- importance:  0.004251612603459441\n",
      "airmax97 ---- importance:  0.0029276956227926847\n",
      "zoom ---- importance:  0.08055805274609852\n",
      "presto ---- importance:  0.021995767571636522\n",
      "airforce ---- importance:  0.013504258636962632\n",
      "blazer ---- importance:  0.014667269976344191\n",
      "vapormax ---- importance:  0.03051001088064317\n",
      "california ---- importance:  0.003283308897902821\n",
      "new_york ---- importance:  0.0032179768300828284\n",
      "oregon ---- importance:  0.0024363060858362184\n",
      "florida ---- importance:  0.002157033520069525\n",
      "texas ---- importance:  0.0019016330912944455\n",
      "other_state ---- importance:  0.0037287576519955466\n",
      "brand2 ---- importance:  0.10333812934393055\n"
     ]
    }
   ],
   "source": [
    "#Print list of feature importances\n",
    "features = x_train_m4.columns\n",
    "importances = list(rf.feature_importances_)\n",
    "for i in range(0,len(importances)):\n",
    "    print(features[i], \"----\", \"importance: \", importances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we run the tuned random forest on the test dataset to get a final performance metric on unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.844611402991486"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = rf.predict(x_test_m4)\n",
    "test_errors = test_preds - y_test_m4\n",
    "test_mape = mean(100 * abs((test_errors/test_preds)))\n",
    "test_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with linear regression, we now create brand separated random forest models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Yeezys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Yeezy train/test data and labels were already created earlier in the notebook when we made the Yeezy linear regression. We have to exponentiate the y-training labels because there is no longer a reason to log-transform **price_ratio**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exponentiating the y_train_yeezy to undo the log-transform for the Yeezy linear model\n",
    "y_train_yeezy = np.exp(y_train_yeezy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the training set into training, validation, and test datasets\n",
    "x_train_yeezy_small, x_valid_yeezy, y_train_yeezy_small, y_valid_yeezy = train_test_split(x_train_yeezy,y_train_yeezy,\\\n",
    "                                                                                          test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output below shows each possible value of the max features parameter and the corresponding MAPE. The best value of max features for the Yeezy random forest is 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 19.09669755274352,\n",
       " 2: 19.09014615469371,\n",
       " 3: 19.031056538661023,\n",
       " 4: 18.952102839481444,\n",
       " 5: 18.887909701582235,\n",
       " 6: 18.875162121887534,\n",
       " 7: 18.908774418789026,\n",
       " 8: 18.9599771989283,\n",
       " 9: 18.995282561423352}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking for the best max_features value \n",
    "max_features_tuning(xtrainset=x_train_yeezy_small, ytrainset=y_train_yeezy_small,\\\n",
    "                    xvalidset=x_valid_yeezy, yvalidset = y_valid_yeezy, trees = 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we rerun the Yeezy random forest with the optimized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=6, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rerun the Yeezy Random forest with the optimal parameters for \n",
    "rf_yeezy = RandomForestRegressor(random_state=np.random.seed(42), max_features=6, n_estimators=100)\n",
    "rf_yeezy.fit(x_train_yeezy, y_train_yeezy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output below contains the feature importances for the Yeezy model. The most important variable is **blackcol**, or whether or not a Yeezy has black in the colorway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shoe Size ---- importance:  0.10645130974951274\n",
      "date_diff ---- importance:  0.3387571568104829\n",
      "V2 ---- importance:  0.15883959192943903\n",
      "blackcol ---- importance:  0.3516795840238471\n",
      "california ---- importance:  0.010299113901521842\n",
      "new_york ---- importance:  0.01182657605608846\n",
      "oregon ---- importance:  0.006210718998793239\n",
      "florida ---- importance:  0.008492757703351663\n",
      "texas ---- importance:  0.007443190826963137\n"
     ]
    }
   ],
   "source": [
    "#Print list of feature importance\n",
    "features = x_train_yeezy.columns\n",
    "importances = list(rf_yeezy.feature_importances_)\n",
    "for i in range(0,len(importances)):\n",
    "    print(features[i], \"----\", \"importance: \", importances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we run the tuned Yeezy random forest on the test dataset to get a final performance metric on unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.934505370655422"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = rf_yeezy.predict(x_test_yeezy)\n",
    "test_errors = test_preds - y_test_yeezy\n",
    "test_mape = mean(100 * abs((test_errors/test_preds)))\n",
    "test_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Off-Whites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same data transformation for the Off-White training labels as we did with the Yeezy training labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exponentiating the y_train_offwhite to undo the log-transform for the Off-White linear model\n",
    "y_train_offwhite = np.exp(y_train_offwhite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we separate the Off-White training data into training and validation for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the training set into training, validation, and test datasets\n",
    "x_train_offwhite_small, x_valid_offwhite, y_train_offwhite_small, y_valid_offwhite = train_test_split(x_train_offwhite,\\\n",
    "                                                                                                      y_train_offwhite,\\\n",
    "                                                                                                      test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output below shows each possible value of the max features parameter and the corresponding MAPE. The best value for the max features in the Off-White random forest is 11. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 12.377834333715402,\n",
       " 2: 12.303493254943696,\n",
       " 3: 12.212604891796738,\n",
       " 4: 12.070428503665593,\n",
       " 5: 12.013451883209292,\n",
       " 6: 11.896210483335043,\n",
       " 7: 11.87443931076127,\n",
       " 8: 11.813600012117684,\n",
       " 9: 11.81376966616931,\n",
       " 10: 11.804110196352854,\n",
       " 11: 11.786658819340762,\n",
       " 12: 11.848103682701485,\n",
       " 13: 11.829993979147238,\n",
       " 14: 11.817021599928449,\n",
       " 15: 11.842236798493627,\n",
       " 16: 11.8415528191091}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking for the best max_features value \n",
    "max_features_tuning(xtrainset=x_train_offwhite_small, ytrainset=y_train_offwhite_small,\\\n",
    "                    xvalidset=x_valid_offwhite, yvalidset = y_valid_offwhite, trees = 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we rerun the optimized Off-White random forest to get the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=11, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rerun the optimized random forest\n",
    "rf_offwhite = RandomForestRegressor(random_state=42,max_features=11, n_estimators=100)\n",
    "rf_offwhite.fit(x_train_offwhite, y_train_offwhite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output below shows the feature importance for the Off-White random forest. The most important variable is **date_diff**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shoe Size ---- importance:  0.0855877255040768\n",
      "date_diff ---- importance:  0.3764687885742764\n",
      "jordan ---- importance:  0.12389757553156164\n",
      "blackcol ---- importance:  0.008417192920324269\n",
      "airmax90 ---- importance:  0.010520696609461446\n",
      "airmax97 ---- importance:  0.007184038541338946\n",
      "zoom ---- importance:  0.17856777823591283\n",
      "presto ---- importance:  0.04041229472619601\n",
      "airforce ---- importance:  0.033698056999216415\n",
      "blazer ---- importance:  0.03587133304719833\n",
      "vapormax ---- importance:  0.0676370102178475\n",
      "california ---- importance:  0.009083758659087712\n",
      "new_york ---- importance:  0.008413527630298727\n",
      "oregon ---- importance:  0.005386898694457153\n",
      "florida ---- importance:  0.00473181826786348\n",
      "texas ---- importance:  0.004121505840882445\n"
     ]
    }
   ],
   "source": [
    "#Print the feature importance\n",
    "features = x_train_offwhite.columns\n",
    "importances = list(rf_offwhite.feature_importances_)\n",
    "#importances\n",
    "for i in range(0,len(importances)):\n",
    "    print(features[i], \"----\", \"importance: \", importances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we run the tuned Off-White random forest on the test dataset to get a final performance metric on unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.644956960532998"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = rf_offwhite.predict(x_test_offwhite)\n",
    "test_errors = test_preds - y_test_offwhite\n",
    "test_mape = mean(100 * abs((test_errors/test_preds)))\n",
    "test_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "----------------\n",
    "In this analysis, we used sneaker transaction data from StockX to understand what makes certain sneakers hype. We found that it is possible to predict the hype of a sneaker, represented by the ratio of sale price to retail price. We were able to determine the most important factors in predicting the hype of a sneaker, such as number of days after release or whether a shoe was a Yeezy Boost 350 V2 or not. StockX can use these results to make decisions on which sneakers to promote to buyers based on how they want to balance maximal revenue with buyer interests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
